# config.yaml
data:
  max_length: 128
  balance_strategy: "undersample"

model:
  base_model: "distilbert/distilbert-base-multilingual-cased"
  num_classes: 3
  dropout_rate: 0.3

training:
  batch_size: 16
  learning_rate: 0.00002
  num_epochs: 3
  warmup_steps: 500
  gradient_clip: 1.0
  early_stopping_patience: 3
  mode: "fine-tune" # Can be "fine-tune" or "frozen"

api:
  host: "0.0.0.0"
  port: 8000
  max_input_length: 512
  top_k_similar: 5

paths:
  processed_data: "data/processed/"
  # Base directory where all training run folders will be saved
  training_output: "training_runs/" 

serving_paths:
  # The API will always load the model and embeddings from here
  models: "training_runs/run_2025-10-20_20-51-31_distilbert-base-multilingual-cased/models"
  embeddings: "training_runs/run_2025-10-20_20-51-31_distilbert-base-multilingual-cased/embeddings"